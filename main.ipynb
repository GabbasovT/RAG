{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b5d1942-9853-41a1-90da-06a4fdbf7500",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:34:15.716118Z",
     "iopub.status.busy": "2025-11-22T13:34:15.715760Z",
     "iopub.status.idle": "2025-11-22T13:34:15.747589Z",
     "shell.execute_reply": "2025-11-22T13:34:15.746826Z",
     "shell.execute_reply.started": "2025-11-22T13:34:15.716094Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gay\n"
     ]
    }
   ],
   "source": [
    "print(\"gay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e57495a1-51d7-4e59-acd2-86bc3436ac24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:34:15.748908Z",
     "iopub.status.busy": "2025-11-22T13:34:15.748580Z",
     "iopub.status.idle": "2025-11-22T13:35:14.672549Z",
     "shell.execute_reply": "2025-11-22T13:35:14.671742Z",
     "shell.execute_reply.started": "2025-11-22T13:34:15.748886Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /usr/local/lib/python3.10/dist-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-11-22 13:35:10.803704: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-22 13:35:11.772822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lib.rag_system import RAGSystem\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9670fb-ab5c-400e-9f7c-687aba8dca3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:35:14.674362Z",
     "iopub.status.busy": "2025-11-22T13:35:14.673705Z",
     "iopub.status.idle": "2025-11-22T13:35:14.689813Z",
     "shell.execute_reply": "2025-11-22T13:35:14.688781Z",
     "shell.execute_reply.started": "2025-11-22T13:35:14.674339Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def demo_custom_rag(index_type=\"hnsw\"):\n",
    "    \"\"\"\n",
    "    Демонстрация работы кастомной RAG системы (DPR + BART)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ДЕМОНСТРАЦИЯ КАСТОМНОЙ RAG СИСТЕМЫ (DPR + BART) с {index_type.upper()} индексом\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    rag = RAGSystem(use_pretrained_rag=False, index_type=index_type)\n",
    "    documents = [\n",
    "        \"Python is a high-level programming language known for its simplicity and readability.\",\n",
    "        \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
    "        \"The Transformer architecture was introduced in the 'Attention is All You Need' paper in 2017.\",\n",
    "        \"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model.\",\n",
    "        \"GPT (Generative Pre-trained Transformer) is designed for text generation tasks.\",\n",
    "        \"Deep learning uses neural networks with multiple layers to learn hierarchical representations.\",\n",
    "        \"Natural Language Processing (NLP) focuses on the interaction between computers and human language.\",\n",
    "        \"RAG combines retrieval and generation for knowledge-intensive tasks.\",\n",
    "        \"DPR (Dense Passage Retrieval) uses dense embeddings for document retrieval.\",\n",
    "        \"BART is a denoising autoencoder for pretraining sequence-to-sequence models.\",\n",
    "        \"Attention mechanisms allow models to focus on relevant parts of the input.\",\n",
    "        \"Fine-tuning adapts pre-trained models to specific downstream tasks.\",\n",
    "        \"Vector databases store and retrieve high-dimensional embeddings efficiently.\",\n",
    "        \"FAISS is a library for efficient similarity search and clustering of dense vectors.\",\n",
    "        \"The encoder-decoder architecture is fundamental to many NLP models.\"\n",
    "    ]\n",
    "\n",
    "    rag.index_documents(documents)\n",
    "    questions = [\n",
    "        \"What is RAG?\",\n",
    "        \"How does DPR work?\",\n",
    "        \"What is BART used for?\"\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        result = rag.answer_question_custom(question, top_k=3)\n",
    "        print(f\"\\n[ОТВЕТ] {result['answer']}\")\n",
    "\n",
    "\n",
    "def demo_pretrained_rag():\n",
    "    \"\"\"\n",
    "    Демонстрация работы предобученной RAG модели\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ДЕМОНСТРАЦИЯ ПРЕДОБУЧЕННОЙ RAG МОДЕЛИ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    rag = RAGSystem()\n",
    "    questions = [\n",
    "        \"Who was the first president of the United States?\",\n",
    "        \"What is the capital of France?\",\n",
    "        \"When was Python created?\"\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        result = rag.answer_question_pretrained(question)\n",
    "        print(f\"\\n[ВОПРОС] {result['question']}\")\n",
    "        print(f\"[ОТВЕТ] {result['answers'][0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff37fcae-5317-471f-8f7c-8dd7cb251b8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:35:14.690869Z",
     "iopub.status.busy": "2025-11-22T13:35:14.690543Z",
     "iopub.status.idle": "2025-11-22T13:36:06.756479Z",
     "shell.execute_reply": "2025-11-22T13:36:06.755544Z",
     "shell.execute_reply.started": "2025-11-22T13:35:14.690848Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ДЕМОНСТРАЦИЯ КАСТОМНОЙ RAG СИСТЕМЫ (DPR + BART) с SIMPLE индексом\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "goida1\n",
      "{'input_ids': tensor([[    3,    31,  7771,  1575,    10,   363,    19,   391,  8418,    58,\n",
      "            31,     6,     3,    31,  1018,  6327,    10,   391,  8418,     3,\n",
      "         15256, 24515,   138,    11,  3381,    21,  1103,    18, 28135,  4145,\n",
      "             5, 20737,    19,     3,     9,   306,    18,  4563,  6020,  1612,\n",
      "           801,    21,   165, 16538,    11,   608,  2020,     5,    37, 23734,\n",
      "            52,    18,   221,  4978,    52,  4648,    19,  4431,    12,   186,\n",
      "           445,  6892,  2250,     5,    31,     6,  1525,    10,     3,    31,\n",
      "             1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "'question: What is RAG?', 'context: RAG combines retrieval and generation for knowledge-intensive tasks. Python is a high-level programming language known for its simplicity and readability. The encoder-decoder architecture is fundamental to many NLP models.', answer: '\n",
      "goida1\n",
      "tensor([[    0,     3, 15256, 24515,   138,    11,  3381,     1]],\n",
      "       device='cuda:0')\n",
      "goida2\n",
      "combines retrieval and generation\n",
      "\n",
      "[ОТВЕТ] combines retrieval and generation\n",
      "\n",
      "================================================================================\n",
      "goida1\n",
      "{'input_ids': tensor([[    3,    31,  7771,  1575,    10,   571,   405,   309,  5554,   161,\n",
      "            58,    31,     6,     3,    31,  1018,  6327,    10,   309,  5554,\n",
      "            41,   308,  5167,  3424,   545,   419,  1788,    15,  2165,    61,\n",
      "          2284, 13809, 25078,    26,    53,     7,    21,  1708, 24515,   138,\n",
      "             5,  5879,  1036,    19,     3,     9,   769,  2244,    13,  7353,\n",
      "          6123,    24,     3,  7161,  1002,    12,   669,    45,   331,     5,\n",
      "         20748, 12009,   995,  2250,    12,   992,    30,  2193,  1467,    13,\n",
      "             8,  3785,     5,    31,     6,  1525,    10,     3,    31,     1]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "'question: How does DPR work?', 'context: DPR (Dense Passage Retrieval) uses dense embeddings for document retrieval. Machine learning is a subset of artificial intelligence that enables systems to learn from data. Attention mechanisms allow models to focus on relevant parts of the input.', answer: '\n",
      "goida1\n",
      "tensor([[    0,  2284, 13809, 25078,    26,    53,     7,    21,  1708, 24515,\n",
      "           138,     1]], device='cuda:0')\n",
      "goida2\n",
      "uses dense embeddings for document retrieval\n",
      "\n",
      "[ОТВЕТ] uses dense embeddings for document retrieval\n",
      "\n",
      "================================================================================\n",
      "goida1\n",
      "{'input_ids': tensor([[    3,    31,  7771,  1575,    10,   363,    19,   272,  8241,   261,\n",
      "            21,    58,    31,     6,     3,    31,  1018,  6327,    10,   272,\n",
      "          8241,    19,     3,     9,   177,    32,  4890,  1510,    35,  4978,\n",
      "            52,    21,   554, 13023,  5932,    18,   235,    18,     7,    15,\n",
      "           835,  3772,  2250,     5,   272, 24203,    41,   279,    23, 26352,\n",
      "           695,  4978,    52,   419, 12640,  1628,    45, 31220,     7,    61,\n",
      "            19,     3,     9,   554,    18,    17, 10761,  1612,   825,     5,\n",
      "         20737,    19,     3,     9,   306,    18,  4563,  6020,  1612,   801,\n",
      "            21,   165, 16538,    11,   608,  2020,     5,    31,     6,  1525,\n",
      "            10,     3,    31,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "'question: What is BART used for?', 'context: BART is a denoising autoencoder for pretraining sequence-to-sequence models. BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model. Python is a high-level programming language known for its simplicity and readability.', answer: '\n",
      "goida1\n",
      "tensor([[    0,   554, 13023,  5932,    18,   235,    18,     7,    15,   835,\n",
      "          3772,  2250,     1]], device='cuda:0')\n",
      "goida2\n",
      "pretraining sequence-to-sequence models\n",
      "\n",
      "[ОТВЕТ] pretraining sequence-to-sequence models\n"
     ]
    }
   ],
   "source": [
    "demo_custom_rag()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
