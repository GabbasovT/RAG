{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b5d1942-9853-41a1-90da-06a4fdbf7500",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T14:45:12.320159Z",
     "iopub.status.busy": "2025-11-24T14:45:12.319303Z",
     "iopub.status.idle": "2025-11-24T14:45:12.338910Z",
     "shell.execute_reply": "2025-11-24T14:45:12.337731Z",
     "shell.execute_reply.started": "2025-11-24T14:45:12.320104Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gay\n"
     ]
    }
   ],
   "source": [
    "print(\"gay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e57495a1-51d7-4e59-acd2-86bc3436ac24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T14:45:12.341882Z",
     "iopub.status.busy": "2025-11-24T14:45:12.340714Z",
     "iopub.status.idle": "2025-11-24T14:46:23.616650Z",
     "shell.execute_reply": "2025-11-24T14:46:23.615507Z",
     "shell.execute_reply.started": "2025-11-24T14:45:12.341833Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /usr/local/lib/python3.10/dist-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-11-24 14:46:16.313322: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-24 14:46:16.373767: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-24 14:46:17.503025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lib.rag_system import RAGSystem\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9670fb-ab5c-400e-9f7c-687aba8dca3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T14:46:23.620076Z",
     "iopub.status.busy": "2025-11-24T14:46:23.618712Z",
     "iopub.status.idle": "2025-11-24T14:46:23.641027Z",
     "shell.execute_reply": "2025-11-24T14:46:23.639934Z",
     "shell.execute_reply.started": "2025-11-24T14:46:23.620028Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def demo_custom_rag(index_type=\"hnsw\"):\n",
    "    \"\"\"\n",
    "    Демонстрация работы кастомной RAG системы (DPR + BART)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ДЕМОНСТРАЦИЯ КАСТОМНОЙ RAG СИСТЕМЫ (DPR + BART) с {index_type.upper()} индексом\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    rag = RAGSystem(use_pretrained_rag=False, index_type=index_type)\n",
    "    documents = [\n",
    "        \"Python is a high-level programming language known for its simplicity and readability.\",\n",
    "        \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
    "        \"The Transformer architecture was introduced in the 'Attention is All You Need' paper in 2017.\",\n",
    "        \"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model.\",\n",
    "        \"GPT (Generative Pre-trained Transformer) is designed for text generation tasks.\",\n",
    "        \"Deep learning uses neural networks with multiple layers to learn hierarchical representations.\",\n",
    "        \"Natural Language Processing (NLP) focuses on the interaction between computers and human language.\",\n",
    "        \"RAG combines retrieval and generation for knowledge-intensive tasks.\",\n",
    "        \"DPR (Dense Passage Retrieval) uses dense embeddings for document retrieval.\",\n",
    "        \"BART is a denoising autoencoder for pretraining sequence-to-sequence models.\",\n",
    "        \"Attention mechanisms allow models to focus on relevant parts of the input.\",\n",
    "        \"Fine-tuning adapts pre-trained models to specific downstream tasks.\",\n",
    "        \"Vector databases store and retrieve high-dimensional embeddings efficiently.\",\n",
    "        \"FAISS is a library for efficient similarity search and clustering of dense vectors.\",\n",
    "        \"The encoder-decoder architecture is fundamental to many NLP models.\"\n",
    "    ]\n",
    "\n",
    "    rag.index_documents(documents)\n",
    "    questions = [\n",
    "        \"What is RAG?\",\n",
    "        \"How does DPR work?\",\n",
    "        \"What is BART used for?\"\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        result = rag.answer_question_custom(question, top_k=3)\n",
    "        print(f\"\\n[ОТВЕТ] {result['answer']}\")\n",
    "\n",
    "\n",
    "def demo_pretrained_rag():\n",
    "    \"\"\"\n",
    "    Демонстрация работы предобученной RAG модели\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ДЕМОНСТРАЦИЯ ПРЕДОБУЧЕННОЙ RAG МОДЕЛИ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    rag = RAGSystem()\n",
    "    questions = [\n",
    "        \"Who was the first president of the United States?\",\n",
    "        \"What is the capital of France?\",\n",
    "        \"When was Python created?\"\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        result = rag.answer_question_pretrained(question)\n",
    "        print(f\"\\n[ВОПРОС] {result['question']}\")\n",
    "        print(f\"[ОТВЕТ] {result['answers'][0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff37fcae-5317-471f-8f7c-8dd7cb251b8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T14:46:23.644210Z",
     "iopub.status.busy": "2025-11-24T14:46:23.642395Z",
     "iopub.status.idle": "2025-11-24T14:47:20.994018Z",
     "shell.execute_reply": "2025-11-24T14:47:20.992857Z",
     "shell.execute_reply.started": "2025-11-24T14:46:23.644158Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ДЕМОНСТРАЦИЯ КАСТОМНОЙ RAG СИСТЕМЫ (DPR + BART) с HNSW индексом\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "combines retrieval and generation\n",
      "\n",
      "[ОТВЕТ] combines retrieval and generation\n",
      "\n",
      "================================================================================\n",
      "uses dense embeddings\n",
      "\n",
      "[ОТВЕТ] uses dense embeddings\n",
      "\n",
      "================================================================================\n",
      "pretraining sequence-to-sequence models\n",
      "\n",
      "[ОТВЕТ] pretraining sequence-to-sequence models\n"
     ]
    }
   ],
   "source": [
    "demo_custom_rag()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afbfd951-d3d3-4c4f-85ef-8cde42b38ee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T14:47:20.998708Z",
     "iopub.status.busy": "2025-11-24T14:47:20.996855Z",
     "iopub.status.idle": "2025-11-24T14:47:21.042788Z",
     "shell.execute_reply": "2025-11-24T14:47:21.041581Z",
     "shell.execute_reply.started": "2025-11-24T14:47:20.998652Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ТЕСТИРОВАНИЕ НА WIKIQA\n",
    "# ============================================================================\n",
    "#\n",
    "# WikiQA - датасет для оценки Question Answering систем\n",
    "# Содержит вопросы и соответствующие документы из Wikipedia\n",
    "#\n",
    "# ИНСТРУКЦИЯ ПО ИСПОЛЬЗОВАНИЮ:\n",
    "# 1. Выполните ЭТУ ЯЧЕЙКУ (определение функций)\n",
    "# 2. Прокрутите вниз к ячейке \"ЗАПУСК ТЕСТИРОВАНИЯ\"\n",
    "# 3. Раскомментируйте нужный режим и запустите\n",
    "#\n",
    "# ДОСТУПНЫЕ РЕЖИМЫ:\n",
    "# - test_wikiqa_simple()              → Быстрый тест (10 примеров)\n",
    "# - test_custom_rag_on_wikiqa()       → Полный тест (20 примеров, все k)\n",
    "# - detailed_analysis()               → Детальный анализ с примерами\n",
    "# - compare_rag_configurations()      → Сравнение HNSW vs Simple\n",
    "# - test_wikiqa_detailed()            → Пошаговое тестирование\n",
    "#\n",
    "# МЕТРИКИ:\n",
    "# - Recall@k    : % вопросов с релевантными документами в top-k\n",
    "# - MRR         : Средний обратный ранг первого релевантного документа\n",
    "# - Precision@k : Средняя точность в top-k результатах\n",
    "#\n",
    "# ⚠️ ВАЖНО: ВЫПОЛНИТЕ ЭТУ ЯЧЕЙКУ ПЕРВОЙ!\n",
    "#\n",
    "\n",
    "def test_wikiqa_simple():\n",
    "    \"\"\"\n",
    "    Простой тест RAG системы на WikiQA датасете\n",
    "    \"\"\"\n",
    "    from lib.wikiqa_evaluation import test_on_wikiqa\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"ТЕСТИРОВАНИЕ RAG СИСТЕМЫ НА WIKIQA\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Создаем RAG систему\n",
    "    rag = RAGSystem(use_pretrained_rag=False, index_type=\"hnsw\")\n",
    "\n",
    "    # Тестируем на WikiQA (используем validation split для быстроты)\n",
    "    metrics = test_on_wikiqa(\n",
    "        rag_system=rag,\n",
    "        split=\"validation\",           # Можно также использовать \"test\"\n",
    "        retrieval_k_values=[1, 3, 5], # Оценим для разных k\n",
    "        generation_top_k=3,            # Используем top-3 для генерации\n",
    "        num_generation_samples=10      # Сгенерируем 10 примеров\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def test_wikiqa_detailed():\n",
    "    \"\"\"\n",
    "    Детальное тестирование с примерами\n",
    "    \"\"\"\n",
    "    from lib.wikiqa_evaluation import WikiQADataset, WikiQAEvaluator\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"ДЕТАЛЬНОЕ ТЕСТИРОВАНИЕ НА WIKIQA\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1. Загружаем датасет\n",
    "    dataset = WikiQADataset(split=\"validation\")\n",
    "    print(f\"\\nЗагружено {len(dataset)} вопросов\")\n",
    "\n",
    "    # 2. Создаем RAG систему\n",
    "    rag = RAGSystem(use_pretrained_rag=False, index_type=\"hnsw\")\n",
    "\n",
    "    # 3. Создаем evaluator и индексируем документы\n",
    "    evaluator = WikiQAEvaluator(rag)\n",
    "    evaluator.prepare_documents(dataset)\n",
    "\n",
    "    # 4. Оценка retrieval\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ОЦЕНКА RETRIEVAL\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for k in [1, 3, 5]:\n",
    "        metrics = evaluator.evaluate_retrieval(dataset, top_k=k)\n",
    "        print(f\"\\nTop-{k} результаты:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "    # 5. Примеры генерации\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ПРИМЕРЫ ГЕНЕРАЦИИ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    results = evaluator.evaluate_generation(dataset, top_k=3, num_samples=5)\n",
    "\n",
    "    for i, sample in enumerate(results['samples'], 1):\n",
    "        print(f\"\\n[Пример {i}]\")\n",
    "        print(f\"Вопрос: {sample['question']}\")\n",
    "        print(f\"Ответ: {sample['generated_answer']}\")\n",
    "        print(f\"\\nИзвлеченные документы:\")\n",
    "        for j, (doc, score) in enumerate(sample['retrieved_docs'][:2], 1):\n",
    "            print(f\"  {j}. [Score: {score:.4f}] {doc[:100]}...\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "\n",
    "def test_custom_rag_on_wikiqa(index_type=\"hnsw\", split=\"validation\"):\n",
    "    \"\"\"\n",
    "    Тестирование кастомной RAG системы (DPR + T5) на WikiQA\n",
    "\n",
    "    Args:\n",
    "        index_type: Тип индекса (\"hnsw\" или \"simple\")\n",
    "        split: Разбиение датасета (\"train\", \"validation\", или \"test\")\n",
    "    \"\"\"\n",
    "    from lib.wikiqa_evaluation import test_on_wikiqa\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ТЕСТИРОВАНИЕ КАСТОМНОЙ RAG СИСТЕМЫ (DPR + T5) НА WIKIQA\")\n",
    "    print(f\"Index: {index_type.upper()}, Split: {split}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Инициализируем RAG систему\n",
    "    rag = RAGSystem(use_pretrained_rag=False, index_type=index_type)\n",
    "\n",
    "    # Тестируем на WikiQA\n",
    "    metrics = test_on_wikiqa(\n",
    "        rag_system=rag,\n",
    "        split=split,\n",
    "        retrieval_k_values=[1, 3, 5, 10],  # Оценим retrieval для разных k\n",
    "        generation_top_k=3,                 # Используем top-3 документа для генерации\n",
    "        num_generation_samples=20           # Сгенерируем 20 примеров\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compare_rag_configurations():\n",
    "    \"\"\"\n",
    "    Сравнение разных конфигураций RAG системы на WikiQA\n",
    "    \"\"\"\n",
    "    from lib.wikiqa_evaluation import WikiQADataset, WikiQAEvaluator\n",
    "    import pandas as pd\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"СРАВНЕНИЕ КОНФИГУРАЦИЙ RAG СИСТЕМЫ НА WIKIQA\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Загружаем датасет один раз\n",
    "    dataset = WikiQADataset(split=\"validation\")\n",
    "\n",
    "    configs = [\n",
    "        {\"name\": \"Custom RAG (HNSW)\", \"use_pretrained_rag\": False, \"index_type\": \"hnsw\"},\n",
    "        {\"name\": \"Custom RAG (Simple)\", \"use_pretrained_rag\": False, \"index_type\": \"simple\"},\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for config in configs:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Тестирование: {config['name']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # Создаем RAG систему\n",
    "        rag = RAGSystem(\n",
    "            use_pretrained_rag=config['use_pretrained_rag'],\n",
    "            index_type=config.get('index_type', 'hnsw')\n",
    "        )\n",
    "\n",
    "        # Создаем evaluator\n",
    "        evaluator = WikiQAEvaluator(rag)\n",
    "\n",
    "        # Индексируем документы\n",
    "        evaluator.prepare_documents(dataset)\n",
    "\n",
    "        # Оценка только retrieval (быстрее)\n",
    "        metrics = {}\n",
    "        for k in [1, 3, 5]:\n",
    "            metrics[k] = evaluator.evaluate_retrieval(dataset, top_k=k)\n",
    "\n",
    "        results[config['name']] = metrics\n",
    "\n",
    "    # Выводим сравнительную таблицу\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"СРАВНИТЕЛЬНАЯ ТАБЛИЦА РЕЗУЛЬТАТОВ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    comparison_data = []\n",
    "    for config_name, k_metrics in results.items():\n",
    "        for k, metrics in k_metrics.items():\n",
    "            comparison_data.append({\n",
    "                'Configuration': config_name,\n",
    "                'k': k,\n",
    "                **metrics\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def detailed_analysis(num_examples=5):\n",
    "    \"\"\"\n",
    "    Детальный анализ работы RAG системы на конкретных примерах\n",
    "    \"\"\"\n",
    "    from lib.wikiqa_evaluation import WikiQADataset, WikiQAEvaluator\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"ДЕТАЛЬНЫЙ АНАЛИЗ RAG СИСТЕМЫ НА WIKIQA\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Загружаем датасет\n",
    "    dataset = WikiQADataset(split=\"validation\")\n",
    "\n",
    "    # Создаем RAG систему\n",
    "    rag = RAGSystem(use_pretrained_rag=False, index_type=\"hnsw\")\n",
    "\n",
    "    # Создаем evaluator и индексируем документы\n",
    "    evaluator = WikiQAEvaluator(rag)\n",
    "    evaluator.prepare_documents(dataset)\n",
    "\n",
    "    # Анализируем несколько примеров\n",
    "    print(f\"\\nДетальный анализ {num_examples} примеров:\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    for i, q_data in enumerate(dataset.questions_data[:num_examples], 1):\n",
    "        print(f\"\\n[ПРИМЕР {i}]\")\n",
    "        print(f\"Вопрос: {q_data['question']}\")\n",
    "        print(f\"\\nРелевантные документы (ground truth):\")\n",
    "        for j, doc in enumerate(q_data['relevant_docs'][:2], 1):\n",
    "            print(f\"  {j}. {doc[:150]}...\")\n",
    "\n",
    "        # Retrieval\n",
    "        retrieved = rag.retrieve(q_data['question'], top_k=5)\n",
    "        print(f\"\\nИзвлеченные документы (Top-5):\")\n",
    "        relevant_docs_set = set(q_data['relevant_docs'])\n",
    "        for j, (doc, score) in enumerate(retrieved, 1):\n",
    "            is_relevant = \"✓ РЕЛЕВАНТЕН\" if doc in relevant_docs_set else \"✗ Не релевантен\"\n",
    "            print(f\"  {j}. [Score: {score:.4f}] {is_relevant}\")\n",
    "            print(f\"     {doc[:150]}...\")\n",
    "\n",
    "        # Генерация\n",
    "        result = rag.answer_question_custom(q_data['question'], top_k=3)\n",
    "        print(f\"\\nСгенерированный ответ:\")\n",
    "        print(f\"  {result['answer']}\")\n",
    "\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c46446a-96a3-4015-b116-ae632db60380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T14:47:21.045804Z",
     "iopub.status.busy": "2025-11-24T14:47:21.044867Z",
     "iopub.status.idle": "2025-11-24T14:50:17.850372Z",
     "shell.execute_reply": "2025-11-24T14:50:17.849180Z",
     "shell.execute_reply.started": "2025-11-24T14:47:21.045754Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ТЕСТИРОВАНИЕ RAG СИСТЕМЫ НА WIKIQA\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка WikiQA датасета (validation split)...\n",
      "Загружено 126 уникальных вопросов\n",
      "Извлечение и индексация документов из WikiQA...\n",
      "Найдено 1119 уникальных документов\n",
      "Индексация завершена!\n",
      "================================================================================\n",
      "ПОЛНАЯ ОЦЕНКА RAG СИСТЕМЫ НА WIKIQA\n",
      "================================================================================\n",
      "\n",
      "Оценка качества retrieval (top_k=1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 126/126 [00:07<00:00, 16.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества retrieval (top_k=3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 126/126 [00:07<00:00, 16.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества retrieval (top_k=5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 126/126 [00:07<00:00, 16.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Генерация ответов (top_k=3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  10%|█         | 1/10 [00:12<01:51, 12.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 6,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  20%|██        | 2/10 [00:13<00:45,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 1957 to 1960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  30%|███       | 3/10 [00:14<00:24,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a product endorser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  40%|████      | 4/10 [00:15<00:14,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strigidae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  50%|█████     | 5/10 [00:16<00:09,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  60%|██████    | 6/10 [00:16<00:06,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952 games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  70%|███████   | 7/10 [00:17<00:03,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  80%|████████  | 8/10 [00:18<00:02,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impact craters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  90%|█████████ | 9/10 [00:19<00:01,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serves the interests of both the general public and private bankers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 10/10 [00:21<00:00,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one or all of the major divisions of the planet's World Ocean\n",
      "\n",
      "================================================================================\n",
      "МЕТРИКИ RETRIEVAL\n",
      "================================================================================\n",
      "\n",
      " k Recall    MRR Precision\n",
      " 1 0.4048 0.4048    0.4048\n",
      " 3 0.6984 0.5357    0.2407\n",
      " 5 0.8095 0.5631    0.1746\n",
      "\n",
      "Легенда:\n",
      "  Recall@k    - % вопросов с релевантными документами в top-k\n",
      "  MRR         - Средний обратный ранг первого релевантного документа\n",
      "  Precision@k - Средняя точность в top-k результатах\n",
      "\n",
      "================================================================================\n",
      "ПРИМЕРЫ ГЕНЕРАЦИИ\n",
      "================================================================================\n",
      "\n",
      "[Пример 1]\n",
      "Вопрос: how big is bmc software in houston, tx\n",
      "Сгенерированный ответ: over 6,000\n",
      "Количество извлеченных документов: 3\n",
      "Первый извлеченный документ: ('Headquartered in Houston , Texas , BMC develops, markets and sells software used for multiple functions, including IT service management, data center automation, performance management, virtualization lifecycle management and cloud computing management.', 0.6961290836334229)...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Пример 2]\n",
      "Вопрос: how long was i love lucy on the air\n",
      "Сгенерированный ответ: from 1957 to 1960\n",
      "Количество извлеченных документов: 3\n",
      "Первый извлеченный документ: ('I Love Lucy was the most watched show in the United States in four of its six seasons, and was the first to end its run at the top of the Nielsen ratings (an accomplishment later matched by The Andy Griffith Show and Seinfeld ).', 0.7010865807533264)...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Пример 3]\n",
      "Вопрос: how did armando christian perez become famous\n",
      "Сгенерированный ответ: a product endorser\n",
      "Количество извлеченных документов: 3\n",
      "Первый извлеченный документ: ('Pérez has become a product endorser, representing alongside Drake , Rihanna , and Trey Songz the “So Kodak” campaign for the Kodak brand and embarking on a partnership with the soft drink giant Dr Pepper as part of the campaign “ Vida 23 ” for which he recorded the song specifically featured on his album Armando .', 0.6337900161743164)...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Пример 4]\n",
      "Вопрос: what bird family is the owl\n",
      "Сгенерированный ответ: Strigidae\n",
      "Количество извлеченных документов: 3\n",
      "Первый извлеченный документ: ('Owls are characterized by their small beaks and wide faces, and are divided into two families : the typical owls , Strigidae; and the barn-owls , Tytonidae.', 0.724579393863678)...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Пример 5]\n",
      "Вопрос: how many people were killed in the oklahoma city bombing\n",
      "Сгенерированный ответ: 168\n",
      "Количество извлеченных документов: 3\n",
      "Первый извлеченный документ: ('The Oklahoma blast claimed 168 lives, including 19 children under the age of 6, and injured more than 680 people.', 0.7417079210281372)...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'retrieval_metrics': {1: {'recall@1': 0.40476190476190477,\n",
       "   'mrr': 0.40476190476190477,\n",
       "   'precision@1': 0.40476190476190477},\n",
       "  3: {'recall@3': 0.6984126984126984,\n",
       "   'mrr': 0.5357142857142857,\n",
       "   'precision@3': 0.2407407407407407},\n",
       "  5: {'recall@5': 0.8095238095238095,\n",
       "   'mrr': 0.563095238095238,\n",
       "   'precision@5': 0.17460317460317457}},\n",
       " 'generation_samples': {'samples': [{'question': 'how big is bmc software in houston, tx',\n",
       "    'generated_answer': 'over 6,000',\n",
       "    'retrieved_docs': [('Headquartered in Houston , Texas , BMC develops, markets and sells software used for multiple functions, including IT service management, data center automation, performance management, virtualization lifecycle management and cloud computing management.',\n",
       "      0.6961290836334229),\n",
       "     ('BMC Software, Inc. is an American company specializing in Business Service Management (BSM) software.',\n",
       "      0.6464785933494568),\n",
       "     ('Employing over 6,000, BMC is often credited with pioneering the BSM concept as a way to help better align IT operations with business needs.',\n",
       "      0.6400122046470642)],\n",
       "    'ground_truth_docs': ['Employing over 6,000, BMC is often credited with pioneering the BSM concept as a way to help better align IT operations with business needs.',\n",
       "     'For 2011, the company recorded an annual revenue of $2.1 billion, making it the #20 largest software company in terms of revenue for that year.']},\n",
       "   {'question': 'how long was i love lucy on the air',\n",
       "    'generated_answer': 'from 1957 to 1960',\n",
       "    'retrieved_docs': [('I Love Lucy was the most watched show in the United States in four of its six seasons, and was the first to end its run at the top of the Nielsen ratings (an accomplishment later matched by The Andy Griffith Show and Seinfeld ).',\n",
       "      0.7010865807533264),\n",
       "     ('After the series ended in 1957, however, a modified version continued for three more seasons with 13 one-hour specials, running from 1957 to 1960, known first as The Lucille Ball-Desi Arnaz Show and later in reruns as The Lucy–Desi Comedy Hour .',\n",
       "      0.6625540852546692),\n",
       "     ('I Love Lucy remains popular, with an American audience of 40 million each year.',\n",
       "      0.6486572027206421)],\n",
       "    'ground_truth_docs': ['The black-and-white series originally ran from October 15, 1951, to May 6, 1957, on the Columbia Broadcasting System (CBS).']},\n",
       "   {'question': 'how did armando christian perez become famous',\n",
       "    'generated_answer': 'a product endorser',\n",
       "    'retrieved_docs': [('Pérez has become a product endorser, representing alongside Drake , Rihanna , and Trey Songz the “So Kodak” campaign for the Kodak brand and embarking on a partnership with the soft drink giant Dr Pepper as part of the campaign “ Vida 23 ” for which he recorded the song specifically featured on his album Armando .',\n",
       "      0.6337900161743164),\n",
       "     (\"In 2005 Pérez and rapper Sean 'Diddy' Combs co-founded Bad Boy Latino , a subsidiary of Combs' Bad Boy Records label.\",\n",
       "      0.623695969581604),\n",
       "     ('He released Rebelution in 2009, which included singles \" I Know You Want Me (Calle Ocho) \" and \" Krazy \".',\n",
       "      0.5840070247650146)],\n",
       "    'ground_truth_docs': ['Armando Pérez (born January 15, 1981), better known by his stage name Pitbull, is an American rapper, songwriter, and record producer.']},\n",
       "   {'question': 'what bird family is the owl',\n",
       "    'generated_answer': 'Strigidae',\n",
       "    'retrieved_docs': [('Owls are characterized by their small beaks and wide faces, and are divided into two families : the typical owls , Strigidae; and the barn-owls , Tytonidae.',\n",
       "      0.724579393863678),\n",
       "     ('Owls are a group of birds that belong to the order Strigiformes, constituting 200 extant bird of prey species .',\n",
       "      0.7191033959388733),\n",
       "     ('Most are solitary and nocturnal , with some exceptions (e.g., the Northern Hawk Owl ).',\n",
       "      0.6475229859352112)],\n",
       "    'ground_truth_docs': ['Owls are a group of birds that belong to the order Strigiformes, constituting 200 extant bird of prey species .']},\n",
       "   {'question': 'how many people were killed in the oklahoma city bombing',\n",
       "    'generated_answer': '168',\n",
       "    'retrieved_docs': [('The Oklahoma blast claimed 168 lives, including 19 children under the age of 6, and injured more than 680 people.',\n",
       "      0.7417079210281372),\n",
       "     ('The Oklahoma City bombing was a domestic terrorist bomb attack on the Alfred P. Murrah Federal Building in downtown Oklahoma City on April 19, 1995.',\n",
       "      0.6813780069351196),\n",
       "     ('On April 19, 2000, the Oklahoma City National Memorial was dedicated on the site of the Murrah Federal Building, commemorating the victims of the bombing.',\n",
       "      0.6704681515693665)],\n",
       "    'ground_truth_docs': ['The Oklahoma blast claimed 168 lives, including 19 children under the age of 6, and injured more than 680 people.']},\n",
       "   {'question': 'how many xbox 360 games are there',\n",
       "    'generated_answer': '952 games',\n",
       "    'retrieved_docs': [('Official Xbox 360 Banner used on game covers from 2010-present',\n",
       "      0.6475847363471985),\n",
       "     ('There are currently 952 games (multiplatform: 751; exclusive: 123; console exclusive: 78) on this list as of April 2, 2013.',\n",
       "      0.6439302563667297),\n",
       "     ('This is a list of retail Xbox 360 games released or planned for release on DVD .',\n",
       "      0.6327551007270813)],\n",
       "    'ground_truth_docs': ['There are currently 952 games (multiplatform: 751; exclusive: 123; console exclusive: 78) on this list as of April 2, 2013.']},\n",
       "   {'question': 'how many planets is jupiter away from the sun?',\n",
       "    'generated_answer': '5',\n",
       "    'retrieved_docs': [('Jupiter is the fifth planet from the Sun and the largest planet in the Solar System .',\n",
       "      0.7136572599411011),\n",
       "     ('Surrounding Jupiter is a faint planetary ring system and a powerful magnetosphere .',\n",
       "      0.6784300804138184),\n",
       "     ('There are also at least 67 moons, including the four large moons called the Galilean moons that were first discovered by Galileo Galilei in 1610.',\n",
       "      0.6717149615287781)],\n",
       "    'ground_truth_docs': ['Jupiter is the fifth planet from the Sun and the largest planet in the Solar System .']},\n",
       "   {'question': 'what happened on the moon during the period of Late Heavy Bombardment?',\n",
       "    'generated_answer': 'impact craters',\n",
       "    'retrieved_docs': [('The Late Heavy Bombardment (commonly referred to as the lunar cataclysm, or LHB) is a hypothetical event around 4.1 to 3.8 billion years ago ( Ga ).',\n",
       "      0.7062027454376221),\n",
       "     ('During this event a very large number of the impact craters on the Moon would have formed, and by inference on Earth , Mercury , Venus and Mars as well.',\n",
       "      0.6437686085700989),\n",
       "     ('The evidence for this event comes primarily from the dating of lunar samples brought back by the Apollo astronauts, which indicates that most impact melt rocks formed in this rather narrow interval of time.',\n",
       "      0.6143931150436401)],\n",
       "    'ground_truth_docs': ['During this event a very large number of the impact craters on the Moon would have formed, and by inference on Earth , Mercury , Venus and Mars as well.']},\n",
       "   {'question': 'what does the federal reserve do',\n",
       "    'generated_answer': 'serves the interests of both the general public and private bankers',\n",
       "    'retrieved_docs': [('Thus the Federal Reserve system has both public and private aspects.',\n",
       "      0.7586019039154053),\n",
       "     ('The Federal Reserve System has both private and public components, and was designed to serve the interests of both the general public and private bankers.',\n",
       "      0.7578846216201782),\n",
       "     (\"The government also exercises some control over the Federal Reserve by appointing and setting the salaries of the system's highest-level employees.\",\n",
       "      0.7563966512680054)],\n",
       "    'ground_truth_docs': [\"Its duties have expanded over the years, and today, according to official Federal Reserve documentation, include conducting the nation's monetary policy , supervising and regulating banking institutions, maintaining the stability of the financial system and providing financial services to depository institutions , the U.S. government, and foreign official institutions.\"]},\n",
       "   {'question': 'how many nature oceans are on earth',\n",
       "    'generated_answer': \"one or all of the major divisions of the planet's World Ocean\",\n",
       "    'retrieved_docs': [(\"In the context of Earth , it refers to one or all of the major divisions of the planet's World Ocean – they are, in descending order of area, the Pacific , Atlantic , Indian , Southern (Antarctic) , and Arctic Oceans.\",\n",
       "      0.6907224655151367),\n",
       "     ('Oceanographers have stated that out of 97%, only 5% of the ocean as a whole on Earth has been explored.',\n",
       "      0.6814075112342834),\n",
       "     ('Earth is the only planet known to have an ocean (or any large amounts of open liquid water).',\n",
       "      0.680359423160553)],\n",
       "    'ground_truth_docs': [\"In the context of Earth , it refers to one or all of the major divisions of the planet's World Ocean – they are, in descending order of area, the Pacific , Atlantic , Indian , Southern (Antarctic) , and Arctic Oceans.\"]}],\n",
       "  'num_evaluated': 10}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ЗАПУСК ТЕСТИРОВАНИЯ WIKIQA - ВЫБЕРИТЕ РЕЖИМ\n",
    "# ============================================================================\n",
    "#\n",
    "# ⚠️ ВАЖНО: Сначала выполните ячейку выше с определениями функций!\n",
    "#\n",
    "\n",
    "# Режим 1: Простой тест (рекомендуется для начала)\n",
    "test_wikiqa_simple()\n",
    "\n",
    "# Режим 2: Полный тест с большим количеством примеров\n",
    "# test_custom_rag_on_wikiqa(index_type=\"hnsw\", split=\"validation\")\n",
    "\n",
    "# Режим 3: Детальный анализ с конкретными примерами\n",
    "# detailed_analysis(num_examples=10)\n",
    "\n",
    "# Режим 4: Сравнение разных конфигураций\n",
    "# compare_rag_configurations()\n",
    "\n",
    "# Режим 5: Детальное тестирование с метриками\n",
    "# test_wikiqa_detailed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b107d-9d4a-461a-8864-26ed131aab36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2da77-76b8-402c-9e41-1fe1b3befb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb0d291-9b06-4769-82d8-f169c92f3ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T14:50:17.853187Z",
     "iopub.status.busy": "2025-11-24T14:50:17.851989Z",
     "iopub.status.idle": "2025-11-24T14:50:17.879236Z",
     "shell.execute_reply": "2025-11-24T14:50:17.878238Z",
     "shell.execute_reply.started": "2025-11-24T14:50:17.853137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom lib.wikiqa_evaluation import WikiQADataset\\n\\ndataset = WikiQADataset(split=\"validation\")\\nquestion_data = dataset[0]  # Первый вопрос\\n\\nprint(f\"Вопрос: {question_data[\\'question\\']}\")\\nprint(f\"Релевантные документы: {len(question_data[\\'relevant_docs\\'])}\")\\nprint(f\"Нерелевантные документы: {len(question_data[\\'irrelevant_docs\\'])}\")\\n\\n# Проверяем что извлекает система\\nrag = RAGSystem(use_pretrained_rag=False, index_type=\"hnsw\")\\nrag.index_documents(dataset.get_all_documents())\\nretrieved = rag.retrieve(question_data[\\'question\\'], top_k=5)\\n\\nfor i, (doc, score) in enumerate(retrieved, 1):\\n    is_relevant = doc in question_data[\\'relevant_docs\\']\\n    print(f\"{i}. [{\\'✓\\' if is_relevant else \\'✗\\'}] {score:.4f}: {doc[:100]}...\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# БЫСТРЫЕ ПРИМЕРЫ И ЭКСПЕРИМЕНТЫ\n",
    "# ============================================================================\n",
    "\n",
    "# Пример 1: Протестировать на другом split (test вместо validation)\n",
    "# test_custom_rag_on_wikiqa(index_type=\"hnsw\", split=\"test\")\n",
    "\n",
    "# Пример 2: Протестировать Simple индекс\n",
    "# test_custom_rag_on_wikiqa(index_type=\"simple\", split=\"validation\")\n",
    "\n",
    "# Пример 3: Больше примеров в детальном анализе\n",
    "# detailed_analysis(num_examples=20)\n",
    "\n",
    "# Пример 4: Анализ конкретного вопроса\n",
    "\"\"\"\n",
    "from lib.wikiqa_evaluation import WikiQADataset\n",
    "\n",
    "dataset = WikiQADataset(split=\"validation\")\n",
    "question_data = dataset[0]  # Первый вопрос\n",
    "\n",
    "print(f\"Вопрос: {question_data['question']}\")\n",
    "print(f\"Релевантные документы: {len(question_data['relevant_docs'])}\")\n",
    "print(f\"Нерелевантные документы: {len(question_data['irrelevant_docs'])}\")\n",
    "\n",
    "# Проверяем что извлекает система\n",
    "rag = RAGSystem(use_pretrained_rag=False, index_type=\"hnsw\")\n",
    "rag.index_documents(dataset.get_all_documents())\n",
    "retrieved = rag.retrieve(question_data['question'], top_k=5)\n",
    "\n",
    "for i, (doc, score) in enumerate(retrieved, 1):\n",
    "    is_relevant = doc in question_data['relevant_docs']\n",
    "    print(f\"{i}. [{'✓' if is_relevant else '✗'}] {score:.4f}: {doc[:100]}...\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dea6503-2d51-47c6-aa5c-822f913f94c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T14:50:17.881757Z",
     "iopub.status.busy": "2025-11-24T14:50:17.880824Z",
     "iopub.status.idle": "2025-11-24T14:53:30.272147Z",
     "shell.execute_reply": "2025-11-24T14:53:30.270906Z",
     "shell.execute_reply.started": "2025-11-24T14:50:17.881710Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ТЕСТИРОВАНИЕ КАСТОМНОЙ RAG СИСТЕМЫ (DPR + T5) НА WIKIQA\n",
      "Index: HNSW, Split: validation\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка WikiQA датасета (validation split)...\n",
      "Загружено 126 уникальных вопросов\n",
      "Извлечение и индексация документов из WikiQA...\n",
      "Найдено 1119 уникальных документов\n",
      "Индексация завершена!\n",
      "================================================================================\n",
      "ПОЛНАЯ ОЦЕНКА RAG СИСТЕМЫ НА WIKIQA\n",
      "================================================================================\n",
      "\n",
      "Оценка качества retrieval (top_k=1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 126/126 [00:07<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества retrieval (top_k=3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 126/126 [00:07<00:00, 16.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества retrieval (top_k=5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 126/126 [00:07<00:00, 16.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка качества retrieval (top_k=10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 126/126 [00:07<00:00, 16.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Генерация ответов (top_k=3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:   5%|▌         | 1/20 [00:10<03:21, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 6,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  10%|█         | 2/20 [00:11<01:29,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from 1957 to 1960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  15%|█▌        | 3/20 [00:12<00:53,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a product endorser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  20%|██        | 4/20 [00:13<00:35,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strigidae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  25%|██▌       | 5/20 [00:14<00:26,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  30%|███       | 6/20 [00:15<00:19,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952 games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  35%|███▌      | 7/20 [00:15<00:14,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  40%|████      | 8/20 [00:17<00:14,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impact craters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  45%|████▌     | 9/20 [00:18<00:13,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serves the interests of both the general public and private bankers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  50%|█████     | 10/20 [00:19<00:13,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one or all of the major divisions of the planet's World Ocean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  55%|█████▌    | 11/20 [00:20<00:11,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two possible values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  60%|██████    | 12/20 [00:22<00:10,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a motion made by a party, during trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  65%|██████▌   | 13/20 [00:22<00:07,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onslow County\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  70%|███████   | 14/20 [00:24<00:07,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the yearly revolution of the Earth around the Sun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  75%|███████▌  | 15/20 [00:25<00:05,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "novels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  80%|████████  | 16/20 [00:26<00:04,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car crash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  85%|████████▌ | 17/20 [00:26<00:02,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  90%|█████████ | 18/20 [00:29<00:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using a lintel , header, or architrave as the horizontal member over a building\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  95%|█████████▌| 19/20 [00:30<00:01,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triiodothyronine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 20/20 [00:31<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unanswerable\n",
      "\n",
      "================================================================================\n",
      "МЕТРИКИ RETRIEVAL\n",
      "================================================================================\n",
      "\n",
      " k Recall    MRR Precision\n",
      " 1 0.4048 0.4048    0.4048\n",
      " 3 0.6984 0.5357    0.2407\n",
      " 5 0.8095 0.5631    0.1746\n",
      "10 0.8730 0.5720    0.0944\n",
      "\n",
      "Легенда:\n",
      "  Recall@k    - % вопросов с релевантными документами в top-k\n",
      "  MRR         - Средний обратный ранг первого релевантного документа\n",
      "  Precision@k - Средняя точность в top-k результатах\n",
      "\n",
      "================================================================================\n",
      "ПРИМЕРЫ ГЕНЕРАЦИИ\n",
      "================================================================================\n",
      "\n",
      "[Пример 1]\n",
      "Вопрос: how big is bmc software in houston, tx\n",
      "Сгенерированный ответ: over 6,000\n",
      "Количество извлеченных документов: 3\n",
      "Первый извлеченный документ: ('Headquartered in Houston , Texas , BMC develops, markets and sells software used for multiple functions, including IT service management, data center automation, performance management, virtualization lifecycle management and cloud computing management.', 0.6961290836334229)...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Пример 2]\n",
      "Вопрос: how long was i love lucy on the air\n",
      "Сгенерированный ответ: from 1957 to 1960\n",
      "Количество извлеченных документов: 3\n",
      "Первый извлеченный документ: ('I Love Lucy was the most watched show in the United States in four of its six seasons, and was the first to end its run at the top of the Nielsen ratings (an accomplishment later matched by The Andy Griffith Show and Seinfeld ).', 0.7010865807533264)...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Пример 3]\n",
      "Вопрос: how did armando christian perez become famous\n",
      "Сгенерированный ответ: a product endorser\n",
      "Количество извлеченных документов: 3\n",
      "Первый извлеченный документ: ('Pérez has become a product endorser, representing alongside Drake , Rihanna , and Trey Songz the “So Kodak” campaign for the Kodak brand and embarking on a partnership with the soft drink giant Dr Pepper as part of the campaign “ Vida 23 ” for which he recorded the song specifically featured on his album Armando .', 0.6337900161743164)...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Пример 4]\n",
      "Вопрос: what bird family is the owl\n",
      "Сгенерированный ответ: Strigidae\n",
      "Количество извлеченных документов: 3\n",
      "Первый извлеченный документ: ('Owls are characterized by their small beaks and wide faces, and are divided into two families : the typical owls , Strigidae; and the barn-owls , Tytonidae.', 0.724579393863678)...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[Пример 5]\n",
      "Вопрос: how many people were killed in the oklahoma city bombing\n",
      "Сгенерированный ответ: 168\n",
      "Количество извлеченных документов: 3\n",
      "Первый извлеченный документ: ('The Oklahoma blast claimed 168 lives, including 19 children under the age of 6, and injured more than 680 people.', 0.7417079210281372)...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'retrieval_metrics': {1: {'recall@1': 0.40476190476190477,\n",
       "   'mrr': 0.40476190476190477,\n",
       "   'precision@1': 0.40476190476190477},\n",
       "  3: {'recall@3': 0.6984126984126984,\n",
       "   'mrr': 0.5357142857142857,\n",
       "   'precision@3': 0.2407407407407407},\n",
       "  5: {'recall@5': 0.8095238095238095,\n",
       "   'mrr': 0.563095238095238,\n",
       "   'precision@5': 0.17460317460317457},\n",
       "  10: {'recall@10': 0.873015873015873,\n",
       "   'mrr': 0.5719986142605191,\n",
       "   'precision@10': 0.09444444444444443}},\n",
       " 'generation_samples': {'samples': [{'question': 'how big is bmc software in houston, tx',\n",
       "    'generated_answer': 'over 6,000',\n",
       "    'retrieved_docs': [('Headquartered in Houston , Texas , BMC develops, markets and sells software used for multiple functions, including IT service management, data center automation, performance management, virtualization lifecycle management and cloud computing management.',\n",
       "      0.6961290836334229),\n",
       "     ('BMC Software, Inc. is an American company specializing in Business Service Management (BSM) software.',\n",
       "      0.6464785933494568),\n",
       "     ('Employing over 6,000, BMC is often credited with pioneering the BSM concept as a way to help better align IT operations with business needs.',\n",
       "      0.6400122046470642)],\n",
       "    'ground_truth_docs': ['Employing over 6,000, BMC is often credited with pioneering the BSM concept as a way to help better align IT operations with business needs.',\n",
       "     'For 2011, the company recorded an annual revenue of $2.1 billion, making it the #20 largest software company in terms of revenue for that year.']},\n",
       "   {'question': 'how long was i love lucy on the air',\n",
       "    'generated_answer': 'from 1957 to 1960',\n",
       "    'retrieved_docs': [('I Love Lucy was the most watched show in the United States in four of its six seasons, and was the first to end its run at the top of the Nielsen ratings (an accomplishment later matched by The Andy Griffith Show and Seinfeld ).',\n",
       "      0.7010865807533264),\n",
       "     ('After the series ended in 1957, however, a modified version continued for three more seasons with 13 one-hour specials, running from 1957 to 1960, known first as The Lucille Ball-Desi Arnaz Show and later in reruns as The Lucy–Desi Comedy Hour .',\n",
       "      0.6625540852546692),\n",
       "     ('I Love Lucy remains popular, with an American audience of 40 million each year.',\n",
       "      0.6486572027206421)],\n",
       "    'ground_truth_docs': ['The black-and-white series originally ran from October 15, 1951, to May 6, 1957, on the Columbia Broadcasting System (CBS).']},\n",
       "   {'question': 'how did armando christian perez become famous',\n",
       "    'generated_answer': 'a product endorser',\n",
       "    'retrieved_docs': [('Pérez has become a product endorser, representing alongside Drake , Rihanna , and Trey Songz the “So Kodak” campaign for the Kodak brand and embarking on a partnership with the soft drink giant Dr Pepper as part of the campaign “ Vida 23 ” for which he recorded the song specifically featured on his album Armando .',\n",
       "      0.6337900161743164),\n",
       "     (\"In 2005 Pérez and rapper Sean 'Diddy' Combs co-founded Bad Boy Latino , a subsidiary of Combs' Bad Boy Records label.\",\n",
       "      0.623695969581604),\n",
       "     ('He released Rebelution in 2009, which included singles \" I Know You Want Me (Calle Ocho) \" and \" Krazy \".',\n",
       "      0.5840070247650146)],\n",
       "    'ground_truth_docs': ['Armando Pérez (born January 15, 1981), better known by his stage name Pitbull, is an American rapper, songwriter, and record producer.']},\n",
       "   {'question': 'what bird family is the owl',\n",
       "    'generated_answer': 'Strigidae',\n",
       "    'retrieved_docs': [('Owls are characterized by their small beaks and wide faces, and are divided into two families : the typical owls , Strigidae; and the barn-owls , Tytonidae.',\n",
       "      0.724579393863678),\n",
       "     ('Owls are a group of birds that belong to the order Strigiformes, constituting 200 extant bird of prey species .',\n",
       "      0.7191033959388733),\n",
       "     ('Most are solitary and nocturnal , with some exceptions (e.g., the Northern Hawk Owl ).',\n",
       "      0.6475229859352112)],\n",
       "    'ground_truth_docs': ['Owls are a group of birds that belong to the order Strigiformes, constituting 200 extant bird of prey species .']},\n",
       "   {'question': 'how many people were killed in the oklahoma city bombing',\n",
       "    'generated_answer': '168',\n",
       "    'retrieved_docs': [('The Oklahoma blast claimed 168 lives, including 19 children under the age of 6, and injured more than 680 people.',\n",
       "      0.7417079210281372),\n",
       "     ('The Oklahoma City bombing was a domestic terrorist bomb attack on the Alfred P. Murrah Federal Building in downtown Oklahoma City on April 19, 1995.',\n",
       "      0.6813780069351196),\n",
       "     ('On April 19, 2000, the Oklahoma City National Memorial was dedicated on the site of the Murrah Federal Building, commemorating the victims of the bombing.',\n",
       "      0.6704681515693665)],\n",
       "    'ground_truth_docs': ['The Oklahoma blast claimed 168 lives, including 19 children under the age of 6, and injured more than 680 people.']},\n",
       "   {'question': 'how many xbox 360 games are there',\n",
       "    'generated_answer': '952 games',\n",
       "    'retrieved_docs': [('Official Xbox 360 Banner used on game covers from 2010-present',\n",
       "      0.6475847363471985),\n",
       "     ('There are currently 952 games (multiplatform: 751; exclusive: 123; console exclusive: 78) on this list as of April 2, 2013.',\n",
       "      0.6439302563667297),\n",
       "     ('This is a list of retail Xbox 360 games released or planned for release on DVD .',\n",
       "      0.6327551007270813)],\n",
       "    'ground_truth_docs': ['There are currently 952 games (multiplatform: 751; exclusive: 123; console exclusive: 78) on this list as of April 2, 2013.']},\n",
       "   {'question': 'how many planets is jupiter away from the sun?',\n",
       "    'generated_answer': '5',\n",
       "    'retrieved_docs': [('Jupiter is the fifth planet from the Sun and the largest planet in the Solar System .',\n",
       "      0.7136572599411011),\n",
       "     ('Surrounding Jupiter is a faint planetary ring system and a powerful magnetosphere .',\n",
       "      0.6784300804138184),\n",
       "     ('There are also at least 67 moons, including the four large moons called the Galilean moons that were first discovered by Galileo Galilei in 1610.',\n",
       "      0.6717149615287781)],\n",
       "    'ground_truth_docs': ['Jupiter is the fifth planet from the Sun and the largest planet in the Solar System .']},\n",
       "   {'question': 'what happened on the moon during the period of Late Heavy Bombardment?',\n",
       "    'generated_answer': 'impact craters',\n",
       "    'retrieved_docs': [('The Late Heavy Bombardment (commonly referred to as the lunar cataclysm, or LHB) is a hypothetical event around 4.1 to 3.8 billion years ago ( Ga ).',\n",
       "      0.7062027454376221),\n",
       "     ('During this event a very large number of the impact craters on the Moon would have formed, and by inference on Earth , Mercury , Venus and Mars as well.',\n",
       "      0.6437686085700989),\n",
       "     ('The evidence for this event comes primarily from the dating of lunar samples brought back by the Apollo astronauts, which indicates that most impact melt rocks formed in this rather narrow interval of time.',\n",
       "      0.6143931150436401)],\n",
       "    'ground_truth_docs': ['During this event a very large number of the impact craters on the Moon would have formed, and by inference on Earth , Mercury , Venus and Mars as well.']},\n",
       "   {'question': 'what does the federal reserve do',\n",
       "    'generated_answer': 'serves the interests of both the general public and private bankers',\n",
       "    'retrieved_docs': [('Thus the Federal Reserve system has both public and private aspects.',\n",
       "      0.7586019039154053),\n",
       "     ('The Federal Reserve System has both private and public components, and was designed to serve the interests of both the general public and private bankers.',\n",
       "      0.7578846216201782),\n",
       "     (\"The government also exercises some control over the Federal Reserve by appointing and setting the salaries of the system's highest-level employees.\",\n",
       "      0.7563966512680054)],\n",
       "    'ground_truth_docs': [\"Its duties have expanded over the years, and today, according to official Federal Reserve documentation, include conducting the nation's monetary policy , supervising and regulating banking institutions, maintaining the stability of the financial system and providing financial services to depository institutions , the U.S. government, and foreign official institutions.\"]},\n",
       "   {'question': 'how many nature oceans are on earth',\n",
       "    'generated_answer': \"one or all of the major divisions of the planet's World Ocean\",\n",
       "    'retrieved_docs': [(\"In the context of Earth , it refers to one or all of the major divisions of the planet's World Ocean – they are, in descending order of area, the Pacific , Atlantic , Indian , Southern (Antarctic) , and Arctic Oceans.\",\n",
       "      0.6907224655151367),\n",
       "     ('Oceanographers have stated that out of 97%, only 5% of the ocean as a whole on Earth has been explored.',\n",
       "      0.6814075112342834),\n",
       "     ('Earth is the only planet known to have an ocean (or any large amounts of open liquid water).',\n",
       "      0.680359423160553)],\n",
       "    'ground_truth_docs': [\"In the context of Earth , it refers to one or all of the major divisions of the planet's World Ocean – they are, in descending order of area, the Pacific , Atlantic , Indian , Southern (Antarctic) , and Arctic Oceans.\"]},\n",
       "   {'question': 'what does a plus-minus sign mean',\n",
       "    'generated_answer': 'two possible values',\n",
       "    'retrieved_docs': [('The sign is normally pronounced \"plus or minus\".',\n",
       "      0.7649275660514832),\n",
       "     ('The plus-minus sign () is a mathematical symbol commonly used either',\n",
       "      0.7638971209526062),\n",
       "     ('In mathematics, it may indicate two possible values: one positive, and one negative.',\n",
       "      0.65582275390625)],\n",
       "    'ground_truth_docs': ['to indicate the precision of an approximation , or',\n",
       "     'to indicate a value that can be of either sign.']},\n",
       "   {'question': 'what does judgment as a matter of law mean',\n",
       "    'generated_answer': 'a motion made by a party, during trial',\n",
       "    'retrieved_docs': [('Judgment as a matter of law (JMOL) is a motion made by a party, during trial, claiming the opposing party has insufficient evidence to reasonably support its case.',\n",
       "      0.7458668947219849),\n",
       "     ('Instead, the judge is said in a j.n.o.v. to be reexamining not the verdict, but his previous rejection of JMOL.',\n",
       "      0.7069057822227478),\n",
       "     ('If there is no evidence to support a reasonable conclusion for the opposing party, judgment is entered by the court and the case is over.',\n",
       "      0.6957308650016785)],\n",
       "    'ground_truth_docs': ['Judgment as a matter of law (JMOL) is a motion made by a party, during trial, claiming the opposing party has insufficient evidence to reasonably support its case.']},\n",
       "   {'question': 'what county is Holly Ridge nc in?',\n",
       "    'generated_answer': 'Onslow County',\n",
       "    'retrieved_docs': [('Holly Ridge is a town in Onslow County , North Carolina , United States .',\n",
       "      0.634557843208313),\n",
       "     ('The Glades is an A&E Network drama television series created by Clifton Campbell.',\n",
       "      0.5731175541877747),\n",
       "     (\"The series stars Matt Passmore as Det. Jim Longworth, a Chicago detective who took a South Florida job with the Florida Department of Law Enforcement (FDLE) after being falsely accused of sleeping with his captain's wife.\",\n",
       "      0.5689488053321838)],\n",
       "    'ground_truth_docs': ['Holly Ridge is a town in Onslow County , North Carolina , United States .']},\n",
       "   {'question': 'what forms seasons',\n",
       "    'generated_answer': 'the yearly revolution of the Earth around the Sun',\n",
       "    'retrieved_docs': [('A season is a subdivision of the year , marked by changes in weather , ecology , and hours of daylight .',\n",
       "      0.7532464861869812),\n",
       "     (\"Seasons result from the yearly revolution of the Earth around the Sun and the tilt of the Earth's axis relative to the plane of revolution.\",\n",
       "      0.7458100914955139),\n",
       "     ('However, ecologists mostly use a six season model for temperate climate regions that includes pre-spring (prevernal) and late summer (serotinal) as distinct seasons along with the traditional four.',\n",
       "      0.7260677218437195)],\n",
       "    'ground_truth_docs': ['A season is a subdivision of the year , marked by changes in weather , ecology , and hours of daylight .',\n",
       "     \"Seasons result from the yearly revolution of the Earth around the Sun and the tilt of the Earth's axis relative to the plane of revolution.\",\n",
       "     'It is the tilt of the Earth that causes the Sun to be higher in the sky during the summer months which increases the solar flux .']},\n",
       "   {'question': 'what kind of literature did john steinbeck writing',\n",
       "    'generated_answer': 'novels',\n",
       "    'retrieved_docs': [('As the author of twenty-seven books, including sixteen novels, six non-fiction books, and five collections of short stories, Steinbeck received the Nobel Prize for Literature in 1962.',\n",
       "      0.6927157640457153),\n",
       "     ('John Ernst Steinbeck, Jr. (February 27, 1902 – December 20, 1968) was an American writer.',\n",
       "      0.6391351222991943),\n",
       "     ('It is considered to be one of the Great American Novels and a treasure of world literature .',\n",
       "      0.6377279162406921)],\n",
       "    'ground_truth_docs': ['As the author of twenty-seven books, including sixteen novels, six non-fiction books, and five collections of short stories, Steinbeck received the Nobel Prize for Literature in 1962.']},\n",
       "   {'question': 'whatever happened clint walker',\n",
       "    'generated_answer': 'car crash',\n",
       "    'retrieved_docs': [('His premature death in a car crash cemented his legendary status.',\n",
       "      0.5351259708404541),\n",
       "     ('The medical examiner mentioned duct tape as one reason she ruled the death a homicide, but officially listed it as \"death by undetermined means\".',\n",
       "      0.5324387550354004),\n",
       "     ('He is most famous for alerting Colonial militia of approaching British forces before the battles of Lexington and Concord , as dramatized in Henry Wadsworth Longfellow \\'s poem, \" Paul Revere\\'s Ride .\"',\n",
       "      0.5299383997917175)],\n",
       "    'ground_truth_docs': ['Norman Eugene Walker, known as Clint Walker (born May 30, 1927), is a retired American actor .']},\n",
       "   {'question': 'what channel is shopnbc on',\n",
       "    'generated_answer': 'CBS',\n",
       "    'retrieved_docs': [('In 2007, it was listed as one of Time magazine\\'s \"100 Best TV Shows of All-TIME.\"',\n",
       "      0.6239960789680481),\n",
       "     ('In February 2008, reruns (edited down to a TV-14 rating) began to air on CBS , although the reruns on CBS ended after one run of the first season.',\n",
       "      0.6072202324867249),\n",
       "     ('Other ventures to expand the brand have included a failed North American adaptation , which aired on MTV in 2011 but it was cancelled after one season after advertisers abandoned the series in response to low ratings and the significant controversy which arose over its depiction of teen sexuality.',\n",
       "      0.595138430595398)],\n",
       "    'ground_truth_docs': ['ShopNBC is an American broadcast and cable home shopping network, owned and operated by ValueVision Media , which is in turn 30% owned by GE Equity and NBC Universal .']},\n",
       "   {'question': 'how post and lintels are used',\n",
       "    'generated_answer': 'using a lintel , header, or architrave as the horizontal member over a building',\n",
       "    'retrieved_docs': [('Post and lintel is different than a beam joined between two posts, it must pass over the tops of the supports.',\n",
       "      0.7310920357704163),\n",
       "     ('Post and lintel, \"prop and lintel\" or \"trabeated\" is a simple construction method using a lintel , header, or architrave as the horizontal member over a building void supported at its ends by two vertical columns , , or .',\n",
       "      0.719641923904419),\n",
       "     ('Post-and-beam is a general term for any kind of timber framing.',\n",
       "      0.6439853310585022)],\n",
       "    'ground_truth_docs': ['Post and lintel, \"prop and lintel\" or \"trabeated\" is a simple construction method using a lintel , header, or architrave as the horizontal member over a building void supported at its ends by two vertical columns , , or .']},\n",
       "   {'question': 'what hormones produce thyroid',\n",
       "    'generated_answer': 'triiodothyronine',\n",
       "    'retrieved_docs': [('The thyroid hormones, triiodothyronine (T3) and thyroxine (T4), are tyrosine -based hormones produced by the thyroid gland that are primarily responsible for regulation of metabolism.',\n",
       "      0.7197831273078918),\n",
       "     ('The major form of thyroid hormone in the blood is thyroxine (T4), which has a longer half-life than T3.',\n",
       "      0.7182310223579407),\n",
       "     ('A deficiency of iodine leads to decreased production of T3 and T4, enlarges the thyroid tissue and will cause the disease known as goitre .',\n",
       "      0.6242861151695251)],\n",
       "    'ground_truth_docs': ['The thyroid hormones, triiodothyronine (T3) and thyroxine (T4), are tyrosine -based hormones produced by the thyroid gland that are primarily responsible for regulation of metabolism.']},\n",
       "   {'question': 'how did James Dean die?',\n",
       "    'generated_answer': 'unanswerable',\n",
       "    'retrieved_docs': [('Dean was the first actor to receive a posthumous Academy Award nomination for Best Actor and remains the only actor to have had two posthumous acting nominations.',\n",
       "      0.6392034292221069),\n",
       "     (\"Dean's enduring fame and popularity rests on his performances in only these three films, all leading roles.\",\n",
       "      0.6081375479698181),\n",
       "     (\"In 1999, the American Film Institute ranked Dean the 18th best male movie star on their AFI's 100 Years...100 Stars list.\",\n",
       "      0.5775817632675171)],\n",
       "    'ground_truth_docs': ['His premature death in a car crash cemented his legendary status.']}],\n",
       "  'num_evaluated': 20}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_custom_rag_on_wikiqa(index_type=\"hnsw\", split=\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f89a26-61c7-4880-8eef-6471e62b102e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7bba35-aa59-4b9e-b2f3-35eb5d397a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199259f-381a-40ab-b6f6-95bdfbabe893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2facb13-a5da-4212-bb6a-8a50c1a81240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349dd73-59e8-4f49-9f71-e90a7be18ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466424b-5829-4257-8215-5e01ea59a2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea65a67-164a-4c19-b23b-8f2590837faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd23c2e6-70c5-477f-a97b-f2300325a280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
